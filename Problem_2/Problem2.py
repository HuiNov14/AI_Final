# -*- coding: utf-8 -*-
"""AI_exercise.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RBdPv7Qsi7zxWyeHAbt0wNM1_pRP0sef
"""

!pip install torchtext==0.17.0
!pip install portalocker
import torch
import torch.nn.functional as F
import torchtext
from torchtext.data.utils import get_tokenizer

# Load the IMDB dataset
train_iter, test_iter = torchtext.datasets.IMDB(split=('train', 'test'))
tokenizer = get_tokenizer('basic_english')

# Hyperparameters
MODEL_NAME = 'imdb-bilstm.model'
EPOCHS = 10
BATCH_SIZE = 64
LEARNING_RATE = 1e-4
EMBEDDING_DIM = 300
HIDDEN_DIM = 128
DROPOUT = 0.5
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(DEVICE)

# Tokenize and sort the data
train_data = [(label, tokenizer(line)) for label, line in train_iter]
train_data.sort(key=lambda x: len(x[1]))
test_data = [(label, tokenizer(line)) for label, line in test_iter]
test_data.sort(key=lambda x: len(x[1]))

def make_vocab(train_data, min_freq):
    vocab = {}
    for label, tokenlist in train_data:
        for token in tokenlist:
            if token not in vocab:
                vocab[token] = 0
            vocab[token] += 1
    vocablist = [('<unk>', 0), ('<pad>', 0), ('<cls>', 0), ('<eos>', 3)]
    vocabidx = {}
    for token, freq in vocab.items():
        if freq >= min_freq:
            idx = len(vocablist)
            vocablist.append((token, freq))
            vocabidx[token] = idx
    vocabidx['<unk>'] = 0
    vocabidx['<pad>'] = 1
    vocabidx['<cls>'] = 2
    vocabidx['<eos>'] = 3
    return vocablist, vocabidx

vocablist, vocabidx = make_vocab(train_data, 10)

def preprocess(data, vocabidx):
    rr = []
    for label, tokenlist in data:
        tkl = ['<cls>']
        for token in tokenlist:
            tkl.append(token if token in vocabidx else '<unk>')
        tkl.append('<eos>')
        rr.append((label, tkl))
    return rr

train_data = preprocess(train_data, vocabidx)
test_data = preprocess(test_data, vocabidx)

def make_batch(data, batchsize):
    bb = []
    blabel = []
    btokenlist = []
    for label, tokenlist in data:
        blabel.append(label)
        btokenlist.append(tokenlist)
        if len(blabel) >= batchsize:
            bb.append((btokenlist, blabel))
            blabel = []
            btokenlist = []
    if len(blabel) > 0:
        bb.append((btokenlist, blabel))
    return bb

train_data = make_batch(train_data, BATCH_SIZE)
test_data = make_batch(test_data, BATCH_SIZE)

def padding(bb):
    for tokenlists, labels in bb:
        maxlen = max([len(x) for x in tokenlists])
        for tkl in tokenlists:
            for i in range(maxlen - len(tkl)):
                tkl.append('<pad>')
    return bb

train_data = padding(train_data)
test_data = padding(test_data)

def word2id(bb, vocbidx):
    rr = []
    for tokenlists, labels in bb:
        id_labels = [label - 1 for label in labels]
        id_tokenlists = []
        for tokenlist in tokenlists:
            id_tokenlists.append([vocabidx[token] for token in tokenlist])
        rr.append((id_tokenlists, id_labels))
    return rr

train_data = word2id(train_data, vocabidx)
test_data = word2id(test_data, vocabidx)

# Load GloVe embeddings
glove_vectors = torchtext.vocab.GloVe(name='6B', dim=EMBEDDING_DIM)

def build_embedding_matrix(vocab, glove_vectors, embedding_dim):
    embedding_matrix = torch.zeros((len(vocab), embedding_dim))
    for word, idx in vocabidx.items():
        if word in glove_vectors.stoi:
            embedding_matrix[idx] = glove_vectors[word]
        else:
            embedding_matrix[idx] = torch.randn(embedding_dim)
    return embedding_matrix

embedding_matrix = build_embedding_matrix(vocablist, glove_vectors, EMBEDDING_DIM)

# Model definition
class MyBiLSTM(torch.nn.Module):
    def __init__(self):
        super(MyBiLSTM, self).__init__()
        vocab_size = len(vocablist)
        self.embedding = torch.nn.Embedding.from_pretrained(embedding_matrix, freeze=False, padding_idx=vocabidx['<pad>'])
        self.lstm = torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True, bidirectional=True)
        self.dropout = torch.nn.Dropout(DROPOUT)
        self.fc = torch.nn.Linear(HIDDEN_DIM * 2, 2)

    def forward(self, x):
        embedded = self.embedding(x)
        lstm_out, (h_n, c_n) = self.lstm(embedded)
        lstm_out = self.dropout(lstm_out[:, -1, :])
        return self.fc(lstm_out)

# Training function
def train():
    model = MyBiLSTM().to(DEVICE)
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    for epoch in range(EPOCHS):
        total_loss = 0
        for tokenlists, labels in train_data:
            tokenlists = torch.tensor(tokenlists, dtype=torch.int64).to(DEVICE)
            labels = torch.tensor(labels, dtype=torch.int64).to(DEVICE)
            optimizer.zero_grad()
            outputs = model(tokenlists)
            loss = F.cross_entropy(outputs, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch {epoch+1}, Loss: {total_loss:.4f}")
    torch.save(model.state_dict(), MODEL_NAME)

# Testing function
def test():
    model = MyBiLSTM().to(DEVICE)
    model.load_state_dict(torch.load(MODEL_NAME))
    model.eval()
    total = 0
    correct = 0
    with torch.no_grad():
        for tokenlists, labels in test_data:
            tokenlists = torch.tensor(tokenlists, dtype=torch.int64).to(DEVICE)
            labels = torch.tensor(labels, dtype=torch.int64).to(DEVICE)
            outputs = model(tokenlists)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    print(f"Accuracy: {correct / total:.4f}")

# Run the training and testing
train()
test()